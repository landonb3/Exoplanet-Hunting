{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exoplanet_Hunting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQaCEc-4t8Fn",
        "colab_type": "text"
      },
      "source": [
        "# Exoplanet Hunting\n",
        "#### Created by Landon Butler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsiXJsvowwaf",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ehtC0BQe5hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import imblearn\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTzRt6GAexJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('/content/exoTrain.csv').fillna(0)\n",
        "test_df = pd.read_csv('/content/exoTest.csv').fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNAFOo67kDnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6ZFmtK_fO7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['Label'] = train_df['LABEL'] - 1\n",
        "test_df['Label'] = test_df['LABEL'] - 1\n",
        "train_df = train_df.drop(columns='LABEL')\n",
        "test_df = test_df.drop(columns='LABEL')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSq3EhM0ujq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y = train_df['Label']\n",
        "train_x = train_df.drop(columns='Label')\n",
        "test_y = test_df['Label']\n",
        "test_x = test_df.drop(columns='Label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn1G2n7zu6YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_labels = train_y.groupby(by=train_y).count()\n",
        "count_labels = count_labels.to_frame(name='Count')\n",
        "count_labels = count_labels.reset_index()\n",
        "g2 = sns.barplot(x='Label', y='Count', data=count_labels,palette=sns.color_palette(\"PRGn\"))\n",
        "g2.set(xticklabels=['No Exoplanet','Contains Exoplanet'])\n",
        "plt.title('Count of Exoplanet Containing Stars')\n",
        "plt.savefig('Count_stars.png',dpi=600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL7eZTRx-DKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exo_train = train_x[train_y == 1].reset_index().melt(id_vars=['index'],var_name=\"Flux Index\", \n",
        "        value_name=\"Flux Intensity\")\n",
        "non_exo_train = train_x[train_y == 0].reset_index().melt(id_vars=['index'],var_name=\"Flux Index\", \n",
        "        value_name=\"Flux Intensity\")\n",
        "print(exo_train.groupby('index').std().mean())\n",
        "print(non_exo_train.groupby('index').std().mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyKyrRijibQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_df = train_x.loc[:4,:'FLUX.200'].reset_index()\n",
        "plot_train_df = plot_train_df.melt(id_vars=['index'],var_name=\"Flux Index\", \n",
        "        value_name=\"Flux Intensity\")\n",
        "sns.set_style(\"dark\")\n",
        "g = sns.lineplot(x=\"Flux Index\", y=\"Flux Intensity\", hue=\"index\",\n",
        "                  data=plot_train_df,legend=False)\n",
        "g.set(xticklabels=[])\n",
        "plt.title('Flux Intensity for Exoplanet Containing Star')\n",
        "plt.savefig('Flux_Intensity.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBA5b-ZOEPxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = train_x.loc[:37,:'FLUX.10'].corr()\n",
        "corr_matrix.style.background_gradient()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viUF91lvq7Gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_exo_df = train_x.loc[50:55,:'FLUX.200'].reset_index()\n",
        "non_exo_df = non_exo_df.melt(id_vars=['index'],var_name=\"Flux Index\", \n",
        "        value_name=\"Flux Intensity\")\n",
        "g1 = sns.lineplot(x=\"Flux Index\", y=\"Flux Intensity\", hue=\"index\",\n",
        "                  data=non_exo_df ,legend=False)\n",
        "g1.set(xticklabels=[])\n",
        "plt.title('Flux Intensity for Non-Exoplanet Containing Star')\n",
        "plt.savefig('Flux_Intensity_NonExo.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya7sX1EiEdYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = train_x.loc[38:1000,:'FLUX.10'].corr()\n",
        "corr_matrix.style.background_gradient()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BWgvSYb0e_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entry = train_x.loc[40,:].to_numpy()\n",
        "plt.plot(entry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyvtq_cHw1o1",
        "colab_type": "text"
      },
      "source": [
        "# Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu7_oUOt_ZA2",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrN2fM2MDiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.ndimage.filters import uniform_filter1d\n",
        "train_x = pd.DataFrame(uniform_filter1d(train_x, axis=1, size=50))\n",
        "test_x = pd.DataFrame(uniform_filter1d(test_x, axis=1, size=50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRuKPUjYOnGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_df = train_x.loc[:4,:'200'].reset_index()\n",
        "plot_train_df = plot_train_df.melt(id_vars=['index'],var_name=\"Flux Index\", \n",
        "        value_name=\"Flux Intensity\")\n",
        "sns.set_style(\"dark\")\n",
        "g = sns.lineplot(x=\"Flux Index\", y=\"Flux Intensity\", hue=\"index\",\n",
        "                  data=plot_train_df,legend=False)\n",
        "g.set(xticklabels=[])\n",
        "plt.title('Filtered Flux Intensity for Exoplanet Containing Star')\n",
        "plt.savefig('Filtered_Flux_Intensity.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ_PLqQeTvph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "\n",
        "train_x = normalize(train_x)\n",
        "test_x = normalize(test_x)\n",
        "\n",
        "train_x = StandardScaler().fit_transform(train_x)\n",
        "test_x = StandardScaler().fit_transform(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTwt-rIWAtpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "four_train_x = np.real(np.fft.fft(train_x,axis=1))\n",
        "four_test_x = np.real(np.fft.fft(test_x,axis=1))\n",
        "\n",
        "plt.plot(four_train_x[0,:])\n",
        "plt.title('Fourier Transform - Star One')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Intensity')\n",
        "plt.savefig('Fourier_Transform.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeLOuOgmFn52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE()\n",
        "train_x_prepped, train_y_prepped = sm.fit_resample(train_x, train_y)\n",
        "test_x_prepped = test_x\n",
        "test_y_prepped = test_y.to_numpy()\n",
        "print(sum(train_y_prepped==0), sum(train_y_prepped==1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZETwbVqUZX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "four_train_x_prepped, four_train_y_prepped = sm.fit_resample(four_train_x, train_y)\n",
        "four_test_x_prepped = four_test_x\n",
        "four_test_y_prepped = test_y.to_numpy()\n",
        "print(sum(four_train_y_prepped==0), sum(four_train_y_prepped==1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPw7EkPe_BTX",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yDcdPJuKrIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.regularizers import L1L2\n",
        "from keras.utils import to_categorical\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.layers import Dense, Conv1D, Flatten,MaxPool1D,BatchNormalization,Dropout\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWiDfnSFKxgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y_prepped_oh = to_categorical(train_y_prepped)\n",
        "test_y_prepped_oh = to_categorical(test_y_prepped)\n",
        "four_train_y_prepped_oh = to_categorical(four_train_y_prepped)\n",
        "four_test_y_prepped_oh = to_categorical(four_test_y_prepped)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4w4LCVAlPxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(2,activation='softmax',\n",
        "                input_dim=train_x_prepped.shape[1]))\n",
        "optimizer = SGD(lr=.01)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "history = model.fit(train_x_prepped, train_y_prepped_oh,epochs=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr2DvKHyMbXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Log_Reg_Accuracy.png',dpi=800)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Log_Reg_Loss.png',dpi=800)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpjxdOHSrhGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = np.argmax(model.predict(test_x_prepped), axis=1)\n",
        "cf_matrix = confusion_matrix(test_y_prepped, y_pred)\n",
        "\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=\"Purples\")\n",
        "plt.title('Logistic Regression Confusion Matrix')\n",
        "plt.savefig('Log_Reg_Con_Mat.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-5oPJafV8P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(2,activation='softmax',\n",
        "                input_dim=train_x_prepped.shape[1]))\n",
        "optimizer = SGD(lr=1)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "history = model.fit(four_train_x_prepped, four_train_y_prepped_oh,epochs=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ClYgBgnieL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set_palette(sns.color_palette(\"tab10\"))\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Fourier_Log_Reg_Accuracy.png',dpi=800)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Fourier_Log_Reg_Loss.png',dpi=800)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9cAfTdxTBRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "four_y_pred = np.argmax(model.predict(four_test_x_prepped), axis=1)\n",
        "cf_matrix = confusion_matrix(test_y, four_y_pred)\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=\"Purples\")\n",
        "plt.title('Logistic Regression Confusion Matrix - Fourier')\n",
        "plt.savefig('Fourier_Log_Reg_Con_Mat.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTiRkC-K_IgG",
        "colab_type": "text"
      },
      "source": [
        "### Feed-forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsnFg_gw_I85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "n_cols = train_x_prepped.shape[1]\n",
        "model.add(Dense(200, activation='relu', input_shape=(n_cols,)))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.000001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_x_prepped, train_y_prepped,epochs=35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXH4EbDAH5ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('FF_Net_Accuracy.png',dpi=800)\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('FF_Net_Loss.png',dpi=800)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewbvDKsAYwrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "y_pred = model.predict(test_x_prepped)\n",
        "fpr, tpr, thresholds = roc_curve(test_y, y_pred)\n",
        "crossover_index = np.min(np.where(1.-fpr <= tpr))\n",
        "crossover_cutoff = thresholds[crossover_index]\n",
        "print(crossover_cutoff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dftz0dAzZf_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred[y_pred < crossover_cutoff] = 0\n",
        "y_pred[y_pred >= crossover_cutoff] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7KTMokYZnIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cf_matrix = confusion_matrix(test_y, y_pred)\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=\"Purples\")\n",
        "plt.title('Feed-Forward Network Confusion Matrix')\n",
        "plt.savefig('FF_Net_Con_Mat.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bce6R1leKQVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "n_cols = train_x_prepped.shape[1]\n",
        "model.add(Dense(200, activation='relu', input_shape=(n_cols,)))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.000001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(four_train_x_prepped, four_train_y_prepped,epochs=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkyTnERsKQfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Log_Reg_Accuracy.png',dpi=800)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Log_Reg_Loss.png',dpi=800)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRaoeU9KYhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(four_test_x_prepped)\n",
        "fpr, tpr, thresholds = roc_curve(test_y, y_pred)\n",
        "crossover_index = np.min(np.where(1.-fpr <= tpr))\n",
        "crossover_cutoff = thresholds[crossover_index]\n",
        "\n",
        "y_pred[y_pred < crossover_cutoff] = 0\n",
        "y_pred[y_pred >= crossover_cutoff] = 1\n",
        "\n",
        "cf_matrix = confusion_matrix(test_y, y_pred)\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=\"Purples\")\n",
        "plt.title('Feed-Forward Fourier Network Confusion Matrix')\n",
        "plt.savefig('Four_FF_Net_Con_Mat.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vnJ8syZ_M2c",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bkYGcvMKKta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "n_cols = train_x_prepped.shape[1]\n",
        "model.add(Conv1D(filters=512, kernel_size=16, activation='relu', input_shape=(n_cols,1)))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Conv1D(filters=256, kernel_size=16, activation='relu'))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Conv1D(filters=128, kernel_size=16, activation='relu'))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Conv1D(filters=64, kernel_size=16, activation='relu'))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.00001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "train_x_prepped_rs = np.expand_dims(train_x_prepped, axis=2)\n",
        "test_x_prepped_rs = np.expand_dims(test_x_prepped, axis=2)\n",
        "class_weight = {0: 1.,\n",
        "                1: 137.}\n",
        "history = model.fit(train_x_prepped_rs, train_y_prepped,epochs=25,class_weight=class_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtE14VkMTevI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Conv_Net_Accuracy.png',dpi=800)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Conv_Net_Loss.png',dpi=800)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPTWOz9uXnMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(test_x_prepped_rs)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(test_y, y_pred)\n",
        "crossover_index = np.min(np.where(1.-fpr <= tpr))\n",
        "crossover_cutoff = thresholds[crossover_index]\n",
        "\n",
        "y_pred[y_pred < crossover_cutoff] = 0\n",
        "y_pred[y_pred >= crossover_cutoff] = 1\n",
        "cf_matrix = confusion_matrix(four_test_y_prepped, y_pred)\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=\"Purples\")\n",
        "plt.title('Convolutional Neural Network Confusion Matrix')\n",
        "plt.savefig('CNN_Con_Mat.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rms2jGXZ3o1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "n_cols = train_x_prepped.shape[1]\n",
        "model.add(Conv1D(filters=512, kernel_size=16, activation='relu', input_shape=(n_cols,1)))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Conv1D(filters=256, kernel_size=16, activation='relu'))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Conv1D(filters=128, kernel_size=16, activation='relu'))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Conv1D(filters=64, kernel_size=16, activation='relu'))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate=0.00001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "four_train_x_prepped_rs = np.expand_dims(four_train_x_prepped, axis=2)\n",
        "four_test_x_prepped_rs = np.expand_dims(four_test_x_prepped, axis=2)\n",
        "class_weight = {0: 1.,\n",
        "                1: 137.}\n",
        "history = model.fit(four_train_x_prepped_rs, train_y_prepped,epochs=25,class_weight=class_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x2NgUmZ3pH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Four_Conv_Net_Accuracy.png',dpi=800)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Four_Conv_Net_Loss.png',dpi=800)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETN-lj9O3pPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "four_test_x_prepped_rs = np.expand_dims(four_test_x_prepped, axis=2)\n",
        "y_pred = model.predict(four_test_x_prepped_rs)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(test_y, y_pred)\n",
        "crossover_index = np.min(np.where(1.-fpr <= tpr))\n",
        "crossover_cutoff = thresholds[crossover_index]\n",
        "\n",
        "y_pred[y_pred < crossover_cutoff] = 0\n",
        "y_pred[y_pred >= crossover_cutoff] = 1\n",
        "cf_matrix = confusion_matrix(four_test_y_prepped, y_pred)\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=\"Purples\")\n",
        "plt.title('Fourier Convolutional Neural Network Confusion Matrix')\n",
        "plt.savefig('Four_CNN_Con_Mat.png',dpi=800)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}